{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f281e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff2c4dc6",
   "metadata": {},
   "source": [
    "First of all, we import the relevant libraries. Besides the wikipedia library, we import devtrans for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b45c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "import devtrans\n",
    "import string\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b298f",
   "metadata": {},
   "source": [
    "Now we set some parameters whih can be easily canged to try new strategies for gathering the data. Since Devanagari text will be converted into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851f81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_chars = string.ascii_letters + \" \" +  \"ред\" + \".\"\n",
    "num_articles = 1500\n",
    "langs = [\"sa\", \"hi\", \"en\", \"ne\"]\n",
    "min_sent_len = 20\n",
    "max_sent_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2141997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_by_value(df, n_rows, col, value):\n",
    "    selected = df[df[col] == value]\n",
    "    selected = selected[0:n_rows]\n",
    "    cut = df[df[col] != value]\n",
    "    return pd.concat([selected, cut])\n",
    "\n",
    "def balance_entries(df, col):\n",
    "    least = min(df[col].value_counts())\n",
    "    for lang in langs:\n",
    "        if df[col].value_counts()[lang] > least:\n",
    "            df = drop_rows_by_value(df, least, \"Tag\", lang)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text, lang):\n",
    "    sentences = []\n",
    "    if lang in (\"sa\", \"hi\", \"ne\"):\n",
    "        text = devtrans.dev2hk(text).replace(\"рд│\", \"\")\n",
    "    cleaned = \"\".join([char for char in converted_to_iast if char in accepted_chars])\n",
    "    sentences += [sentence for sentence in re.split(\"ред|\\.\", cleaned) if len(sentence) > min_sent_len and len(sentence) < max_sent_len]\n",
    "    return sentences\n",
    "\n",
    "def download_articles(lang, how_many):\n",
    "    wiki.set_lang(lang)\n",
    "    articles = []\n",
    "    tags = []\n",
    "    topics = wiki.random(how_many)\n",
    "    for topic in topics:\n",
    "        try:\n",
    "            sentences = process(wiki.WikipediaPage(topic).content, lang)\n",
    "            for sentence in sentences:\n",
    "                tags.append(lang)\n",
    "            articles += sentences\n",
    "        except (wiki.PageError, wiki.DisambiguationError):\n",
    "            print(\"Unable to retrieve content for: \" + topic)\n",
    "    return (articles, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e0804",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentences = []\n",
    "final_tags = []\n",
    "sents = []\n",
    "tags = []\n",
    "\n",
    "for lang in langs:\n",
    "    final_sentences.append(download_articles(lang, num_articles))\n",
    "\n",
    "for group in final_sentences:\n",
    "    for i, sent in enumerate(group[0]):\n",
    "        sents.append(sent)\n",
    "        tags.append(group[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce032462",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = {\"Sentence\":sents, \"Tag\":tags}\n",
    "df = balance_entries(pd.DataFrame(zipped), \"Tag\")\n",
    "\n",
    "X, y = df.iloc[:, 0], df.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a41870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb40f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range=(1,3), analyzer=\"char\")\n",
    "\n",
    "pipe_1r_r13 = pipeline.Pipeline([(\"vectorizer\", vectorizer),\n",
    "                                 (\"clf\", linear_model.LogisticRegression())])\n",
    "\n",
    "pipe_1r_r13.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = pipe_1r_r13.predict(X_test)\n",
    "\n",
    "acc = (metrics.accuracy_score(y_test, y_predicted)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cde60",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = [\"This is a string\", \"mujhe mAluM hai ki tumhArI\", \"lakSyatAvacchedakAvacchinnaM na bhavati\", \"pramAnalakSaNaM syAt\", \"My name is something or other\", \"These things are all nice\", \"pramANaprameyasaMzayaprayojanadRSTAntAdi\", \"Dave went to sleep at hai hai hai merA\", \"pratiyogitA kA nAma kyA zuklatvam iti hai\"]\n",
    "\n",
    "new_pred = pipe_1r_r13.predict(new_test)\n",
    "print(new_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181b66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
